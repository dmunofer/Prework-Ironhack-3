{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4a98624a",
      "metadata": {
        "id": "4a98624a"
      },
      "source": [
        "# Recall Machine Learning Linear Regression\n",
        "\n",
        "At the end of this Lesson the studen will remember the main steps to train a model:\n",
        "\n",
        " - Split dataset in train and test subsets\n",
        " - Standardize continuous varuables\n",
        " - Transform categorical variables to dummy\n",
        " - Train linear regression models\n",
        " - Train classification models\n",
        " - Interpret the error and accuracy metrics to validate the built models\n",
        "\n",
        "**You have two exercises at the end of the notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7589307d",
      "metadata": {
        "id": "7589307d"
      },
      "source": [
        "### Import data and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "23ad14c6",
      "metadata": {
        "id": "23ad14c6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e2e209b3",
      "metadata": {
        "scrolled": true,
        "id": "e2e209b3",
        "outputId": "6bcebd6c-7803-4dee-aee9-1a19751f9cf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c851cd0815c8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..\\data\\Fish.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\data\\\\Fish.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('..\\data\\Fish.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b223673d",
      "metadata": {
        "id": "b223673d"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc62392a",
      "metadata": {
        "id": "bc62392a"
      },
      "source": [
        "### Species variable treatment\n",
        "\n",
        "Species is a categorical variable, hence we need to transform it to dummies before inserting in the model\n",
        "\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a419c133",
      "metadata": {
        "id": "a419c133"
      },
      "outputs": [],
      "source": [
        "df.Species.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0614800",
      "metadata": {
        "id": "e0614800"
      },
      "source": [
        "Firstly, let's reduce the categories to Perch, Bream and Others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e973527c",
      "metadata": {
        "scrolled": true,
        "id": "e973527c"
      },
      "outputs": [],
      "source": [
        "def fish_species(x):\n",
        "    if x == 'Perch':\n",
        "        return 'Perch'\n",
        "    elif x == 'Bream':\n",
        "        return 'Bream'\n",
        "    else:\n",
        "        return 'Others'\n",
        "\n",
        "df['fish_species'] = df['Species'].apply(fish_species)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01a42172",
      "metadata": {
        "id": "01a42172"
      },
      "source": [
        "### Get dummies\n",
        "\n",
        "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
        "\n",
        "https://stats.stackexchange.com/questions/350492/why-do-we-create-dummy-variables\n",
        "\n",
        "https://towardsdatascience.com/what-are-dummy-variables-and-how-to-use-them-in-a-regression-model-ee43640d573e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3a7ff6a",
      "metadata": {
        "scrolled": true,
        "id": "a3a7ff6a"
      },
      "outputs": [],
      "source": [
        "df_dum = pd.get_dummies(df.fish_species)\n",
        "df = df.merge(df_dum, right_index = True, left_index = True, how = 'left')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00222b8b",
      "metadata": {
        "scrolled": true,
        "id": "00222b8b"
      },
      "outputs": [],
      "source": [
        "df.drop(['Species','fish_species'], axis = 1, inplace = True)\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38c2a911",
      "metadata": {
        "id": "38c2a911"
      },
      "source": [
        "### Train test split\n",
        "\n",
        "It is mandatory to randomly divide the dataset into two. One for training the model and the test split for validate it.\n",
        "\n",
        "If error metrics are low with the test split means that our model is robust\n",
        "\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02e7faad",
      "metadata": {
        "id": "02e7faad"
      },
      "outputs": [],
      "source": [
        "fish_train, fish_test = train_test_split(df, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce85406a",
      "metadata": {
        "id": "ce85406a"
      },
      "outputs": [],
      "source": [
        "print(fish_train.shape)\n",
        "print(fish_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cb0b22a",
      "metadata": {
        "id": "1cb0b22a"
      },
      "source": [
        "### Standardize the numerical variables\n",
        "\n",
        "Sometimes numerical variables in our dataset have very different scales, taht's to have very different values between one column and other. That can harm model accuracy.\n",
        "\n",
        "For solve this situation, we standardize, that's to put every continuous variable centered in 0 and with standard deviation 1\n",
        "\n",
        "We **first** standardize the training set, then the test set with the training set parameters\n",
        "\n",
        "We do not standardize the target variable\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler\n",
        "\n",
        "\n",
        "https://www.askpython.com/python/examples/standardize-data-in-python#:~:text=Ways%20to%20Standardize%20Data%20in%20Python%201%201.,load_iris%20...%202%202.%20Using%20StandardScaler%20%28%29%20function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "556f9391",
      "metadata": {
        "id": "556f9391"
      },
      "outputs": [],
      "source": [
        "scale= StandardScaler()\n",
        "\n",
        "variables_sc = ['duration_ms', 'loudness', 'tempo']\n",
        "\n",
        "scale_fit = scale.fit(X_train1[variables_sc])\n",
        "\n",
        "X_train_sc = pd.DataFrame(scale.transform(X_train1[variables_sc]), columns = variables_sc)\n",
        "\n",
        "X_test_sc = pd.DataFrame(scale.transform(X_test1[variables_sc]), columns = variables_sc)\n",
        "\n",
        "X_train_sc.shape\n",
        "\n",
        "X_train1.drop(variables_sc, axis = 1, inplace = True)\n",
        "X_train1 = X_train1.reset_index(drop = True)\n",
        "y_train1 = y_train1.reset_index(drop=True)\n",
        "X_train = pd.concat([X_train1, X_train_sc], axis = 1)\n",
        "\n",
        "X_test1.drop(variables_sc, axis = 1, inplace = True)\n",
        "X_test1 = X_test1.reset_index(drop = True)\n",
        "y_test1 = y_test1.reset_index(drop=True)\n",
        "X_test = pd.concat([X_test1, X_test_sc], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d1580d5",
      "metadata": {
        "id": "4d1580d5"
      },
      "outputs": [],
      "source": [
        "scale= StandardScaler()\n",
        "variables_sc = ['Length1', 'Length2', 'Length3', 'Height', 'Width']\n",
        "\n",
        "X_train = fish_train[['Length1', 'Length2', 'Length3', 'Height', 'Width', 'Bream','Others', 'Perch']]\n",
        "y_train  = fish_train['Weight']\n",
        "\n",
        "X_test = fish_test[['Length1', 'Length2', 'Length3', 'Height', 'Width', 'Bream', 'Others', 'Perch']]\n",
        "y_test  = fish_test['Weight']\n",
        "\n",
        "\n",
        "scale_train = scale.fit(X_train[variables_sc])\n",
        "\n",
        "X_train_sc = pd.DataFrame(scale_train.transform(X_train[variables_sc]), columns = [variables_sc])\n",
        "X_train = X_train.drop(variables_sc, axis = 1) # , inplace = True\n",
        "X_train = X_train.reset_index(drop = True)\n",
        "X_train = pd.concat([X_train, X_train_sc], axis = 1)\n",
        "X_train.columns = ['Length1', 'Length2', 'Length3', 'Height', 'Width', 'Bream','Others', 'Perch']\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "X_test_sc = pd.DataFrame(scale_train.transform(X_test[variables_sc]), columns =[variables_sc])\n",
        "X_test = X_test.drop(variables_sc, axis = 1) # , inplace = True\n",
        "X_test = X_test.reset_index(drop = True)\n",
        "X_test = pd.concat([X_test, X_test_sc], axis = 1)\n",
        "X_test.columns = ['Length1', 'Length2', 'Length3', 'Height', 'Width', 'Bream','Others', 'Perch']\n",
        "y_test = y_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d3a411c",
      "metadata": {
        "id": "3d3a411c"
      },
      "source": [
        "## Linear Regression\n",
        "\n",
        "https://medium.com/swlh/interpreting-linear-regression-through-statsmodels-summary-4796d359035a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53e13378",
      "metadata": {
        "scrolled": false,
        "id": "53e13378"
      },
      "outputs": [],
      "source": [
        "X_train = sm.add_constant(X_train)\n",
        "result = sm.OLS(y_train, X_train).fit()\n",
        "\n",
        "print(result.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d33fde23",
      "metadata": {
        "scrolled": true,
        "id": "d33fde23"
      },
      "outputs": [],
      "source": [
        "X_train['predict'] = result.predict(X_train)\n",
        "\n",
        "X_test = sm.add_constant(X_test)\n",
        "X_test['predict'] = result.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "216f039c",
      "metadata": {
        "id": "216f039c"
      },
      "outputs": [],
      "source": [
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return np.mean(np.abs((sum(y_true) - sum(y_pred)) / sum(y_true))) * 100\n",
        "\n",
        "print(\"MAE: \", metrics.mean_absolute_error(y_train, X_train['predict']))\n",
        "print(\"MSE: \", metrics.mean_squared_error(y_train, X_train['predict']))\n",
        "print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_train, X_train['predict'])))\n",
        "print(\"MAPE: \", mean_absolute_percentage_error(y_train, X_train['predict']))\n",
        "print(\"R2: \", metrics.r2_score(y_train, X_train['predict']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8b4c39d",
      "metadata": {
        "id": "e8b4c39d"
      },
      "outputs": [],
      "source": [
        "print(\"MAE: \", metrics.mean_absolute_error(y_test, X_test['predict']))\n",
        "print(\"MSE: \", metrics.mean_squared_error(y_test, X_test['predict']))\n",
        "print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, X_test['predict'])))\n",
        "print(\"MAPE: \", mean_absolute_percentage_error(y_test, X_test['predict']))\n",
        "print(\"R2: \", metrics.r2_score(y_test, X_test['predict']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbbaee41",
      "metadata": {
        "id": "bbbaee41"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Response the answers in 4-5 lines each, read the links you have along this document, or in the theory notebooks, or you can also search on the internet:\n",
        "\n",
        " - Which type of variables do we transform into dummies? Why do we do it?\n",
        " - Why is so important to divide our data into train and test datasets? Which is the purpose of doing it?\n",
        " - Why do we standardize some varaiables? Which type of variables do we standardize?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las variables que transformamos en dummies son las categóricas. Lo hacemos para conservar la información de las categorías originales evitando errores"
      ],
      "metadata": {
        "id": "pOaGD4mBaNpG"
      },
      "id": "pOaGD4mBaNpG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "La razón por la que separamos los datos en train y test es poder entrenar con una cantidad de datos y luego validar con los datos que faltan. Lo hacemos para evitar overfitting"
      ],
      "metadata": {
        "id": "Jx-4FaJOaNkf"
      },
      "id": "Jx-4FaJOaNkf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estandarizamos las variables para evitar errores cuando las variables estan en diferentes escalas lo que puede llevarnos a una predicción menos precisa. Estandarizamos variables numéricas"
      ],
      "metadata": {
        "id": "85SWZrjGaNeS"
      },
      "id": "85SWZrjGaNeS"
    },
    {
      "cell_type": "markdown",
      "id": "42f23018",
      "metadata": {
        "id": "42f23018"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "Regarding the summary and the errors, Would you use this model to predict the weights of the fishes? Justify your answer. Comment the usefulness of the main indicators of the summary and the errors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si, porque según las gráficas, podemos observar una relación lineal entre el peso de los peces y las demás variables"
      ],
      "metadata": {
        "id": "Ma8_L28PbwCL"
      },
      "id": "Ma8_L28PbwCL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "El resumen nos ayuda a visualizar los datos del modelo, la variable que queremos predecir y la matriz de covarianza del error. Por lo que podemos observar el error de cada uno de los métodos para seleccionar el más óptimo"
      ],
      "metadata": {
        "id": "HTvT247Abv6_"
      },
      "id": "HTvT247Abv6_"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}